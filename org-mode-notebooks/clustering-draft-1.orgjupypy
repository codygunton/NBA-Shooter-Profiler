#+TODO: TODO | RUN
* RUN Imports and config

#+begin_src jupyter-python
import pandas as pd
import numpy as np

import seaborn as sns

from sklearn.cluster import KMeans
from sklearn.cluster import DBSCAN
from sklearn import decomposition
from sklearn import preprocessing
from sklearn import metrics
from sklearn.utils import resample

# for silhouette analysis
import matplotlib.pyplot as plt
from sklearn.metrics import silhouette_samples, silhouette_score
import matplotlib.cm as cm
#+end_src

#+RESULTS:

Set up seaborn
#+begin_src jupyter-python :results none
sns.set(context="notebook",
  rc={"font.size":12, "font.family":"sans-serif", 
      "font.serif":"P052", # note: kills the original list font.serif
      "axes.titlesize":"large", "axes.titlepad":20, 
      "axes.titleweight":"bold",
      "axes.labelsize":"medium", "axes.labelpad":10, 
      "axes.labelweight":"light",  # lighter is not actually an option
      "xtick.labelsize":"small", "ytick.labelsize":"small", 
#      "axes.facecolor":"white", # vertical axis padding is werid with this
      "legend.fontsize": "medium",
      "xtick.major.width":3, "ytick.major.width":3,
      "legend.loc":"best",
      "figure.dpi":200, "figure.titlesize":"large", 
      "figure.titleweight":"bold"
  }
)

context: "notebook"
#+end_src



#+begin_src jupyter-python
df = pd.read_csv("../data/shot_logs_prepped.csv")
print(df.columns)

cluster_vars = ["SHOT_CLOCK", 
                "DRIBBLES", 
                "TOUCH_TIME", 
                "SHOT_DIST"]
#+end_src

#+RESULTS:
: Index(['GAME_ID', 'W', 'SHOT_CLOCK', 'GAME_CLOCK', 'DRIBBLES', 'TOUCH_TIME',
:        'SHOT_DIST', 'player_name'],
:       dtype='object')

* Specify player and build standardized data frame
** RUN Specify player and define pf = their data frame
Specify a player and define player frame.
#+begin_src jupyter-python
player_name = "stephen curry"
pf = df[df.player_name == player_name]
print(f"Player: {player_name.title()}\n", f"Number of shots: {pf.shape[0]}")
#+end_src

#+RESULTS:
: Player: Stephen Curry
:  Number of shots: 965

** PCA to compare dribbles and touch time
Dribbles and touch time are definitely correlated, but we would like
to understand now strong this effect is.

#+begin_src jupyter-python
pca = decomposition.PCA()
pca.fit(pf[["DRIBBLES", "TOUCH_TIME"]])
pca.get_covariance()
#+end_src

#+RESULTS:
: array([[17.79588932, 13.79556748],
:        [13.79556748, 11.72825634]])

** RUN Standardize pf

#+begin_src jupyter-python
stzd_cluster_vars = pd.DataFrame(preprocessing.scale(pf[cluster_vars]), 
                                 columns=cluster_vars, index=pf.index)
d = dict([[c, stzd_cluster_vars[c]] for c in cluster_vars])
pf_stzd = pf.assign(**d)
#+end_src

#+RESULTS:

* Cluster
** k-Means
*** Validclust 

NOTE: I don't trust this package. Also: can't control seed?

NOTE: I got a FutureWarning saying calinski_harabaz_score will be
renamed to calinski_harabasz_score. I fixed this with an easy replace
in two files in the validclust package.

NOTE: You definitely see the effects of standardization here;
look at heatmap.

#+begin_src jupyter-python :file cluster_validation_heatmap.png
from validclust.validclust import ValidClust
vclust = ValidClust(k=list(range(2, 8)), 
                    # had to remove 'silhouette' index bc of errors
                    # modified package to get calinski to work (see above)
                    indices=['calinski', 'dunn', 'davies'
],  
                    methods=['hierarchical', 'kmeans']
)
cvi_vals = vclust.fit(pf_stzd[cluster_vars])
vclust.plot()
print(vclust._normalize().to_string())
# print(cvi_vals.to_string())
# k = vclust._normalize().loc["kmeans","davies"].idxmax(axis=1)
#+end_src

#+RESULTS:
:RESULTS:
:                               2         3         4         5         6         7
: method       index                                                               
: hierarchical calinski  0.850061  0.827757  0.825549  0.939317  0.953766  1.000000
:              dunn      0.760976  0.760976  0.972336  0.846028  0.846028  1.000000
:              davies   -1.000000 -0.721137 -0.416103 -0.113158  0.100436  0.241307
: kmeans       calinski  0.833388  0.818177  0.911861  0.948331  0.995315  1.000000
:              dunn      0.930504  0.764770  1.000000  0.467424  0.356706  0.673853
:              davies   -1.000000 -0.548309 -0.078869  0.098479  0.324365  0.456802
[[file:cluster_validation_heatmap.png]]
:END:


From documentation: note that, because the scores are normalized along
each method/index pair, you should compare the colors of the cells in
the heatmap only within a given row.

*** Davies-Bouldin score
#+begin_src jupyter-python
rows = []
for k in range(2, 8):
    kmeans = KMeans(n_clusters = k, random_state=1).fit(pf_stzd[cluster_vars])
    pf_stzd = pf_stzd.assign(CLUSTER=kmeans.labels_)
    score = metrics.davies_bouldin_score(pf_stzd[cluster_vars], pf_stzd.CLUSTER)
    rows += [score]

DB_comp = pd.DataFrame(rows, columns = 
                       ["DB_SCORE"], index=range(2,8))
DB_comp
#+end_src

#+RESULTS:
:RESULTS:
|   | DB_SCORE |
|---+----------|
| 2 | 1.270709 |
| 3 | 1.148432 |
| 4 | 1.021171 |
| 5 | 0.974754 |
| 6 | 0.915406 |
| 7 | 0.870258 |
:END:

*** Silhouette analysis
   
This is derived from the code [[https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html][here]] in the sklearn documentation. It produces silhouette plots, which give a finer-grained approach to model selection than simple use of average silhouette score.
#+begin_src jupyter-python
X = pf_stzd[cluster_vars]

range_n_clusters = range(2,8)
num_ks = len(range_n_clusters)

fig, axs = plt.subplots(nrows=len(range_n_clusters), sharex=True,
                         figsize=(4,2*num_ks),
                        gridspec_kw={"hspace":0.1},
)


fig.suptitle("Silhouette Plots for k-Means\nPlayer: {}"
             .format(player_name.title()), y=0.935, fontsize="large")

for n_clusters in range_n_clusters:
    ax1 = axs[n_clusters - 2]

    # The (n_clusters+1)*10 is for inserting blank space between silhouette
    # plots of individual clusters, to demarcate them clearly.
    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])

    # Initialize the clusterer with n_clusters value and a random generator
    # seed of 10 for reproducibility.
    clusterer = KMeans(n_clusters=n_clusters, random_state=1)
    cluster_labels = clusterer.fit_predict(X)

    # The silhouette_score gives the average value for all the samples.
    # This gives a perspective into the density and separation of the formed
    # clusters
    silhouette_avg = silhouette_score(X, cluster_labels)
    print("For n_clusters =", n_clusters,
          "The average silhouette_score is :", silhouette_avg)

    # Compute the silhouette scores for each sample
    sample_silhouette_values = silhouette_samples(X, cluster_labels)

    y_lower = 10
    colors = cm.get_cmap("Set1").colors
    for i in range(n_clusters,-1,-1):
        # Aggregate the silhouette scores for samples belonging to
        # cluster i, and sort them
        ith_cluster_silhouette_values = \
            sample_silhouette_values[cluster_labels == i]

        ith_cluster_silhouette_values.sort()

        size_cluster_i = ith_cluster_silhouette_values.shape[0]
        y_upper = y_lower + size_cluster_i

        color = colors[i]
        ax1.fill_betweenx(np.arange(y_lower, y_upper),
                          0, ith_cluster_silhouette_values,
                          facecolor=color, edgecolor=color, alpha=1)

        # # Label the silhouette plots with their cluster numbers at the middle
        # ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))

        # Compute the new y_lower for next plot
        y_lower = y_upper + 10  # 10 for the 0 samples


   # now set up axes
#     ax1.set_title("k={}; player: {}".format(n_clusters, player_name.title()))
#    ax1.set_xlabel("Silhouette coeff. values")
    ax1.set_ylabel("k = {}".format(n_clusters))

    # The vertical line for average silhouette score of all the values
    ax1.axvline(x=silhouette_avg, color="red", linestyle="--")

    ax1.set_yticks([])  # Clear the yaxis labels / ticks
    ax1.set_xticks(np.arange(-0.1,1,0.2))
#    ax1.set_facecolor("xkcd:lime")
#    ax1.margins(y=.1)
#+end_src

#+RESULTS:
:RESULTS:
: For n_clusters = 2 The average silhouette_score is : 0.32401213227528014
: For n_clusters = 3 The average silhouette_score is : 0.33157612295300487
: For n_clusters = 4 The average silhouette_score is : 0.34173652888379913
: For n_clusters = 5 The average silhouette_score is : 0.3381908062752307
: For n_clusters = 6 The average silhouette_score is : 0.35970738857231094
: For n_clusters = 7 The average silhouette_score is : 0.3758830782406466
[[file:./.ob-jupyter/c3cb8fc2d25a95c7efb8a624dd23ecb5bc48203f.png]]
:END:
*** Calinski-Harabasz score

Out of curiosity, I look at how much the C-H score changes depending
on whether we use the standardized vs the unstandardized data.
#+begin_src jupyter-python
rows = []
CH_score = metrics.calinski_harabasz_score
for k in range(2,8):
    kmeans = KMeans(n_clusters = k, random_state=1).fit(pf_stzd[cluster_vars])
    
    pf_cl = pf.assign(CLUSTER=kmeans.labels_)
    score = CH_score(pf_cl[cluster_vars], pf_cl.CLUSTER)

    pf_stzd = pf_stzd.assign(CLUSTER=kmeans.labels_)
    score_stzd = CH_score(pf_stzd[cluster_vars], pf_stzd.CLUSTER)

    rows += [[score, score_stzd]]

CH_comp = pd.DataFrame(rows, columns = 
                       ["unstandardized", "standardized"], index=range(2,8))
CH_comp
#+end_src

#+RESULTS:
:RESULTS:
|   | unstandardized | standardized |
|---+----------------+--------------|
| 2 | 186.542309     | 461.480369   |
| 3 | 643.198937     | 453.057254   |
| 4 | 646.428730     | 504.931150   |
| 5 | 442.413149     | 525.095129   |
| 6 | 583.788271     | 551.183841   |
| 7 | 594.776273     | 554.876103   |
:END:

*** RUN k-means

Just choosing a k for Curry at the moment.
#+begin_src jupyter-python
k = 4
#+end_src

#+RESULTS:

 Run k-means, attach cluster variables, and separate wins and losses.
#+begin_src jupyter-python :results none
clustering = KMeans(n_clusters = k, random_state=1).fit(pf_stzd[cluster_vars])
#+end_src

#+begin_src jupyter-python
 pf_cl = pf.assign(CLUSTER=clustering.labels_)
 pfwins_cl = pf_cl[pf_cl.W=="W"]
 pflosses_cl = pf_cl[pf_cl.W=="L"]

 print("Number of wins:   {}\nNumber of losses: {}"
       .format(pfwins_cl.shape[0], pflosses_cl.shape[0]))
#+end_src

 #+RESULTS:
 : Number of wins:   766
 : Number of losses: 199

** DBSCAN
*** Choose epsilon using Silhouette score
#+begin_src jupyter-python
X = pf_stzd[cluster_vars]

idx = pd.Index([], name="eps")
results = pd.DataFrame([], index=idx, columns=["num. clusters", "score"])

for eps in np.arange(0.05, 0.85, 0.02):
    db = DBSCAN(eps).fit(X)
    # count labels, including -1 for "unlabeled"
    num_clusters = pd.Series(db.labels_).nunique()
    if num_clusters > 1:
        pf_cl = pf.assign(CLUSTER=db.labels_)
        score = metrics.silhouette_score(pf_cl[cluster_vars], 
                                             pf_cl.CLUSTER)
        results.loc[eps] = [num_clusters, score]

print(results.sort_values("score", ascending=False).head(10))
#+end_src

#+RESULTS:
#+begin_example
      num. clusters     score
eps                          
0.83            3.0  0.312761
0.81            3.0  0.311319
0.79            3.0  0.311319
0.63            3.0  0.186968
0.73            4.0  0.185146
0.61            3.0  0.184687
0.71            4.0  0.183967
0.59            3.0  0.182917
0.57            3.0  0.181090
0.55            3.0  0.179085
#+end_example

*** cluster 
#+begin_src jupyter-python
eps = results.score.idxmax()

db = DBSCAN(eps).fit(X)
# count labels, including -1 for "unlabeled"
num_clusters = pd.Series(db.labels_).nunique()
if num_clusters > 1:
    pf_cl = pf.assign(CLUSTER=db.labels_)
    score = metrics.silhouette_score(pf_cl[cluster_vars], 
                                         pf_cl.CLUSTER)
#+end_src

#+RESULTS:
:RESULTS:
|       | GAME_ID  | W   | SHOT_CLOCK | GAME_CLOCK | DRIBBLES | TOUCH_TIME | SHOT_DIST | player_name   | CLUSTER |
|-------+----------+-----+------------+------------+----------+------------+-----------+---------------+---------|
| 13559 | 21400907 | W   | 16.3       | 589        | 2        | 4.0        | 8.0       | stephen curry | 0       |
| 13560 | 21400907 | W   | 16.6       | 446        | 0        | 2.4        | 25.9      | stephen curry | 0       |
| 13561 | 21400907 | W   | 11.0       | 334        | 0        | 0.9        | 23.8      | stephen curry | 0       |
| 13562 | 21400907 | W   | 18.5       | 300        | 2        | 3.4        | 27.5      | stephen curry | 0       |
| 13563 | 21400907 | W   | 3.0        | 11         | 1        | 1.5        | 25.1      | stephen curry | 0       |
| ...   | ...      | ... | ...        | ...        | ...      | ...        | ...       | ...           | ...     |
| 14519 | 21400014 | W   | 19.7       | 361        | 7        | 5.1        | 3.2       | stephen curry | 0       |
| 14520 | 21400014 | W   | 3.1        | 210        | 12       | 8.7        | 6.5       | stephen curry | -1      |
| 14521 | 21400014 | W   | 3.7        | 135        | 4        | 4.8        | 23.4      | stephen curry | 0       |
| 14522 | 21400014 | W   | 3.3        | 93         | 0        | 0.6        | 25.0      | stephen curry | 0       |
| 14523 | 21400014 | W   | 12.2       | 34         | 15       | 12.3       | 25.6      | stephen curry | 0       |

965 rows × 9 columns
:END:

Separate wins and losses:
#+begin_src jupyter-python
pfwins_cl = pf_cl[pf_cl.W=="W"]
pflosses_cl = pf_cl[pf_cl.W=="L"]

print("Number of wins:   {}\nNumber of losses: {}"
       .format(pfwins_cl.shape[0], pflosses_cl.shape[0]))
#+end_src

#+RESULTS:
: Number of wins:   766
: Number of losses: 199

* Plot clusters
** Set up global axis specification functions

#+begin_src jupyter-python
cluster_var_lims1 = list(map(lambda v: [df[str(v)].min(), df[str(v)].max()], cluster_vars))
cluster_var_lims2 = cluster_var_lims1.copy()
cluster_var_lims2.reverse()

cluster_var_lims = list(map(lambda v: [df[str(v)].min(), df[str(v)].max()], cluster_vars))
cluster_var_lims

# these limits are bad for someone like Anthony Davis

#will add a little padding to make the plots look nicer
ep = 1

cluster_var_lims = list(map(lambda L: [L[0]-ep, np.round(L[1]+ep,2)], cluster_var_lims))
cluster_var_lims
#+end_src

#+RESULTS:
| -1.0 | 25.0 |
|   -1 |   33 |
| -1.0 | 25.9 |
| -1.0 | 48.2 |

Now we will get x limits and y limits that will worth for every
possible pairgrid.
#+begin_src jupyter-python :results none
import itertools
prod_lims = list(itertools.product(cluster_var_lims, repeat=2))
prod_names = list(itertools.product(cluster_vars, repeat=2))

# to agree with the arrangement of plots in the PairGrid, shuld swap each pair
swap = lambda L: L[::-1]
prod_lims = list(map(swap, prod_lims))
prod_names = list(map(swap, prod_names))

# now get 2-d array of tuples of xlims
xlims = np.empty(len(cluster_vars)**2, dtype=object)
xlims[:] = [tuple(t[0]) for t in prod_lims]
xlims = xlims.reshape((len(cluster_vars),len(cluster_vars)))

# now get 2-d array of tuples of ylims
ylims = np.empty(len(cluster_vars)**2, dtype=object)
ylims[:] = [tuple(t[1]) for t in prod_lims]
ylims = ylims.reshape((len(cluster_vars),len(cluster_vars)))
#+end_src

#+begin_src jupyter-python :results none
def safe_set_xlim(xlim, ax):
    if ax is None:
        pass
    else:
        ax.set_xlim(xlim)
vec_safe_set_xlim = np.vectorize(safe_set_xlim)

def safe_set_ylim(ylim, ax):
    if ax is None:
        pass
    else:
        ax.set_ylim(ylim)
vec_safe_set_ylim = np.vectorize(safe_set_ylim)
#+end_src

** Set up per-player axis specs

#+begin_src jupyter-python
cluster_var_lims1 = list(map(lambda v: [pf[str(v)].min(), pf[str(v)].max()], cluster_vars))
cluster_var_lims2 = cluster_var_lims1.copy()
cluster_var_lims2.reverse()

cluster_var_lims = list(map(lambda v: [pf[str(v)].min(), pf[str(v)].max()], cluster_vars))
cluster_var_lims

#will add a little padding to make the plots look nicer
ep = 1

cluster_var_lims = list(map(lambda L: [L[0]-ep, np.round(L[1]+ep,2)], cluster_var_lims))
cluster_var_lims
#+end_src

#+RESULTS:
| -0.9 | 25.0 |
|   -1 |   24 |
| -1.0 | 21.9 |
| -0.9 | 43.7 |

Now we will get x limits and y limits that will work for every
possible pairgrid.
#+begin_src jupyter-python :results none
import itertools
prod_lims = list(itertools.product(cluster_var_lims, repeat=2))
prod_names = list(itertools.product(cluster_vars, repeat=2))

# to agree with the arrangement of plots in the PairGrid, shuld swap each pair
swap = lambda L: L[::-1]
prod_lims = list(map(swap, prod_lims))
prod_names = list(map(swap, prod_names))

# now get 2-d array of tuples of xlims
xlims = np.empty(len(cluster_vars)**2, dtype=object)
xlims[:] = [tuple(t[0]) for t in prod_lims]
xlims = xlims.reshape((len(cluster_vars),len(cluster_vars)))

# now get 2-d array of tuples of ylims
ylims = np.empty(len(cluster_vars)**2, dtype=object)
ylims[:] = [tuple(t[1]) for t in prod_lims]
ylims = ylims.reshape((len(cluster_vars),len(cluster_vars)))
#+end_src

#+begin_src jupyter-python :results none
def safe_set_xlim(xlim, ax):
    if ax is None:
        pass
    else:
        ax.set_xlim(xlim)
vec_safe_set_xlim = np.vectorize(safe_set_xlim)

def safe_set_ylim(ylim, ax):
    if ax is None:
        pass
    else:
        ax.set_ylim(ylim)
vec_safe_set_ylim = np.vectorize(safe_set_ylim)
#+end_src

** Plot the wins and losses pairplots


#+begin_src jupyter-python :file all.png
plot_df = pf_cl[cluster_vars+["CLUSTER"]]

# note: there is a known issue with KDE estimating bandwidth to 0
# it can be resolved by manually setting bw, e.g.,, by passing diag_kws={'bw':0.1},
# but that results in ugly plots sometimes.
g = sns.pairplot(plot_df, hue="CLUSTER", corner=True, 
# to resolve KDE failure:
#diag_kws={'bw':0.6}
# or else
# diag_kind="hist", diag_kws={"histtype":"step"}
)
vec_safe_set_xlim(xlims, g.axes)
vec_safe_set_ylim(ylims, g.axes)

g.fig.suptitle("{} All Shots".format(player_name.title()), x=0.7, y=0.985, fontsize="xx-large")
g._legend.set_bbox_to_anchor((0.9, 0.7))
#+end_src

#+RESULTS:
[[file:all.png]]


#+begin_src jupyter-python :file w.png
plot_df = pfwins_cl[cluster_vars+["CLUSTER"]]

# note: there is a known issue with KDE estimating bandwidth to 0
# it can be resolved by manually setting bw, e.g.,, by passing diag_kws={'bw':0.1},
# but that results in ugly plots sometimes.
g = sns.pairplot(plot_df, hue="CLUSTER", corner=True, 
# to resolve KDE failure:
#diag_kws={'bw':0.6}
# or else
# diag_kind="hist", diag_kws={"histtype":"step"}
)
vec_safe_set_xlim(xlims, g.axes)
vec_safe_set_ylim(ylims, g.axes)

g.fig.suptitle("{} in Wins".format(player_name.title()), x=0.7, y=0.985, fontsize="xx-large")
g._legend.set_bbox_to_anchor((0.9, 0.7))
#+end_src

#+RESULTS:
[[file:w.png]]

#+begin_src jupyter-python :file l.png
plot_df = pflosses_cl[cluster_vars+["CLUSTER"]]
# could restrict to only some clusters like this
# plot_df = plot_df[plot_df.CLUSTER.isin([1,2])]

# note: there is a known issue with KDE estimating bandwidth to 0
# it can be resolved by manually setting bw, e.g.,, by passing diag_kws={'bw':0.1},
# but that results in ugly plots sometimes.
g = sns.pairplot(plot_df, hue="CLUSTER", corner=True, 
# to resolve KDE failure:
# diag_kws={'bw':0.6}
# or else
# diag_kind="hist", diag_kws={"histtype":"step"}
)
vec_safe_set_xlim(xlims, g.axes)
vec_safe_set_ylim(ylims, g.axes)
g.axes[0][0].yaxis.set_label_text("butt")

g.fig.suptitle("{} in Losses".format(player_name.title()), x=0.7, y=0.985, fontsize="xx-large")
g._legend.set_bbox_to_anchor((0.9, 0.7))
#+end_src

#+RESULTS:
[[file:l.png]]


** Plot just one cluster

#+begin_src jupyter-python :results none
i = 0
pf_cl_single = pf_cl[pf_cl.CLUSTER==0]
#+end_src

#+begin_src jupyter-python :file one-cluster-win-or-loss.png
plot_df = pf_cl_single[cluster_vars+["W"]]

# note: there is a known issue with KDE estimating bandwidth to 0
# it can be resolved by manually setting bw, e.g.,, by passing diag_kws={'bw':0.1},
# but that results in ugly plots sometimes.
g = sns.pairplot(plot_df, hue="W", corner=True, 
# to resolve KDE failure:
#diag_kws={'bw':0.6}
# or else
#diag_kind="hist", diag_kws={"histtype":"step"}
)
vec_safe_set_xlim(xlims, g.axes)
vec_safe_set_ylim(ylims, g.axes)

g.fig.suptitle("{} cluster {}".format(player_name.title(), i), x=0.7, y=0.985, fontsize="xx-large")
g._legend.set_bbox_to_anchor((0.9, 0.7))
#+end_src

#+RESULTS:
[[file:one-cluster-win-or-loss.png]]


* Compare clusters
** Compare within-cluster mean with bootstrap

This is _very_ quick-and-dirty. I ended up abandoning this approach to focus on using permutations.

#+begin_src jupyter-python
i=0
B = 1000

wins = pfwins_cl.query("CLUSTER=={}".format(i))[cluster_vars]
losses = pflosses_cl.query("CLUSTER=={}".format(i))[cluster_vars]
d_test = wins.mean()-losses.mean()
test_stat = np.inner(d_test, d_test) 
## ^ need to check this ^

bs_ds = [] # samples of the distance between the means
for b in range(B):
    bs_wins = resample(wins)
    bs_losses = resample(losses)
    d = bs_wins.mean()-bs_losses.mean()
    bs_ds += [np.inner(d,d)]
bs_ds = pd.Series(bs_ds)
#+end_src

#+RESULTS:


#+begin_src jupyter-python
# whether distance between means is zero.
def loc_bs_cluster(i, B):
    wins = pfwins_cl.query("CLUSTER=={}".format(i))[cluster_vars]
    losses = pflosses_cl.query("CLUSTER=={}".format(i))[cluster_vars]
    d_test = wins.mean()-losses.mean()
    test_stat = np.inner(d_test, d_test) 
    ## ^ need to check this ^

    bs_ds = [] # samples of the distance between the means
    for b in range(B):
        bs_wins = resample(wins)
        bs_losses = resample(losses)
        d = bs_wins.mean()-bs_losses.mean()
        bs_ds += [np.inner(d,d)]
    bs_ds = pd.Series(bs_ds)
    return test_stat, bs_ds
#+end_src

#+RESULTS:

#+begin_src jupyter-python
%%time

from statsmodels.distributions.empirical_distribution import ECDF
B = 1000
for i in pf_cl.CLUSTER.unique():
    print("Cluster: {}".format(i))
    
    test_stat, bs = loc_bs_cluster(i, B)
    mu = bs.mean()
    bs_cdf = ECDF(bs)
    pval = 1 - bs_cdf(test_stat)
    print("p = {}\n".format(pval))
#+end_src

#+RESULTS:
#+begin_example
Cluster: 0
p = 0.616

Cluster: 1
p = 0.981

Cluster: 3
p = 0.6659999999999999

Cluster: 2
p = 0.7969999999999999

CPU times: user 6.53 s, sys: 3.16 ms, total: 6.54 s
Wall time: 6.55 s
#+end_example

** Compare within-cluster mean with permutations

#+begin_src jupyter-python
# whether distance between means is zero.
def cluster_loc_perm(i, num_perms):
    wins = pfwins_cl.query("CLUSTER=={}".format(i))[cluster_vars]
    losses = pflosses_cl.query("CLUSTER=={}".format(i))[cluster_vars]
    d_test = wins.mean()-losses.mean()
    test_stat = np.inner(d_test, d_test) 
    ## ^ need to check this ^

    ds = [] # samples of the distance between the means
    for n in range(num_perms):
        both = np.concatenate([wins, losses])
        both = np.random.permutation(both)
        vir_wins, vir_losses = np.split(both, [len(wins)])
        d = vir_wins.mean(axis=0) - vir_losses.mean(axis=0)
        ds += [np.inner(d,d)]
    ds = pd.Series(ds)
    return test_stat, ds
#+end_src

#+RESULTS:

#+begin_src jupyter-python
%%time

from statsmodels.distributions.empirical_distribution import ECDF

N = 1000
for i in pf_cl.CLUSTER.unique():
    print("Cluster: {}".format(i))
    
    test_stat, perm_ds = cluster_loc_perm(i, N)
    mu = perm_ds.mean()
    print(f"Test stat:{test_stat}; mu:{mu}")
    perm_ds_cdf = ECDF(perm_ds)

    if mu < test_stat:
        pval = 1 - perm_ds_cdf(test_stat)
    else:
        pval = perm_ds_cdf(test_stat)

    print("p = {}\n".format(pval))
#+end_src

#+RESULTS:
#+begin_example
Cluster: 0
Test stat:2.2633830335566545; mu:0.8722861821674186
p = 0.06099999999999994

Cluster: 1
Test stat:0.032315314354755806; mu:0.43619263391750335
p = 0.018000000000000002

Cluster: 3
Test stat:1.372831372344848; mu:1.3793458755611097
p = 0.615

Cluster: 2
Test stat:10.242451159977422; mu:12.478055726463385
p = 0.547

CPU times: user 767 ms, sys: 6 µs, total: 767 ms
Wall time: 763 ms
#+end_example

** Permutation analysis of Hotelling statistic
*** Scratch for one cluster



#+begin_src jupyter-python
i = 0
wins = pfwins_cl.query("CLUSTER=={}".format(i))[cluster_vars]
wins.to_csv("./wins.csv", index=False)
losses = pflosses_cl.query("CLUSTER=={}".format(i))[cluster_vars]
losses.to_csv("./losses.csv", index=False)
#+end_src

#+RESULTS:

**** using org-babel embedded R
[[https://cran.r-project.org/web/packages/Hotelling/Hotelling.pdf][Documentation for Hotelling library]].

Did I actually need to run setwd("~/Dropbox/ML/nba-shots/")?
#+begin_src R :session
library(Hotelling)
wins <- read.csv(file="./wins.csv")
losses <- read.csv(file="./losses.csv")

T2 <- hotelling.stat(wins, losses)
#+end_src

#+RESULTS:
| 8.68027805225357 | 0.247377622377622 |   4 | 223 | 65 | 4 |
| 8.68027805225357 | 0.247377622377622 | 283 | 223 | 65 | 4 |


Really, the output is a list where the component df is a vector of length 2.
#+begin_src R :session
T2$df
#+end_src

#+RESULTS:
|   4 |
| 283 |

This is what we're after.

#+begin_src R :session
T2$statistic
#+end_src

#+RESULTS:
: 8.68027805225357
Check below that I get the same below using rpy2.

**** using rpy2
#+begin_src jupyter-python
import rpy2.robjects as ro
from rpy2.robjects.packages import importr
from rpy2.robjects import pandas2ri
from rpy2.robjects.conversion import localconverter
#+end_src


#+begin_src jupyter-python
with localconverter(ro.default_converter + pandas2ri.converter):
  r_wins = ro.conversion.py2ri(wins)
  r_losses = ro.conversion.py2ri(losses)
r_losses
#+end_src

#+RESULTS:
:RESULTS:
R/rpy2 DataFrame (65 x 4)
| SHOT_CLOCK | DRIBBLES | TOUCH_TIME | SHOT_DIST |
|------------+----------+------------+-----------|
| 8.000000   | 3        | 5.200000   | 9.300000  |
| 18.700000  | 7        | 5.200000   | 4.200000  |
| 12.000000  | 3        | 4.500000   | 6.200000  |
| 17.600000  | 8        | 6.900000   | 8.500000  |
| ...        | ...      | ...        | ...       |
| 22.900000  | 1        | 1.800000   | 0.600000  |
| 18.200000  | 3        | 4.100000   | 3.900000  |
| 15.100000  | 4        | 3.000000   | 11.300000 |
| 16.600000  | 3        | 3.000000   | 3.300000  |
:END:

#+begin_src jupyter-python
import rpy2.robjects as robjects
hotelling = importr("Hotelling")
T2 = robjects.r["hotelling.stat"]
#+end_src

#+RESULTS:

#+begin_src jupyter-python
r_stat = T2(r_wins, r_losses)
r_stat
#+end_src

#+RESULTS:
:RESULTS:
ListVector with 6 elements.
| statistic | FloatVector with 1 elements. |
|           | | 8.680278 |                 |
| m         | FloatVector with 1 elements. |
|           | | 0.247378 |                 |
| df        | FloatVector with 2 elements. |
|           | | 4.000000 | 283.000000 |    |
| nx        | IntVector with 1 elements.   |
|           | | 223 |                      |
| ny        | IntVector with 1 elements.   |
|           | | 65 |                       |
| p         | IntVector with 1 elements.   |
|           | | 4 |                        |
:END:

#+begin_src jupyter-python
x = ro.conversion.ri2py(r_stat[0])
list(x)
#+end_src

#+RESULTS:
| 8.680278052253561 |

*** permutations for one cluster using rpy2
Now make a function and hope the overhead of converting many times is acceptable.

Set up df's for a single cluster, again.

Set up interactions with R.
#+begin_src jupyter-python
from rpy2.robjects.packages import importr
from rpy2.robjects import pandas2ri
from rpy2.robjects.conversion import localconverter
import rpy2.robjects as ro

importr("Hotelling")
r_T2 = ro.r["hotelling.stat"]
#+end_src

#+RESULTS:

#+begin_src jupyter-python
# takes in two pandas data frames and produces floating number = T^2 value.
def T2(wins, losses):
    with localconverter(ro.default_converter + pandas2ri.converter):
        r_wins = ro.conversion.py2ri(wins)
        r_losses = ro.conversion.py2ri(losses)
        # should find better way to unpack; float() doesn't work
    T2_val = list(r_T2(r_wins, r_losses)[0])[0]
    return T2_val

# eg to check against what's above:
i=0
wins = pfwins_cl.query("CLUSTER=={}".format(i))[cluster_vars]
losses = pflosses_cl.query("CLUSTER=={}".format(i))[cluster_vars]
T2(wins, losses)

both = np.concatenate([wins, losses])
both = np.random.permutation(both)
vir_wins, vir_losses = map(pd.DataFrame, np.split(both, [len(wins)]))

with localconverter(ro.default_converter + pandas2ri.converter):
    r_wins = ro.conversion.py2ri(wins)
    r_losses = ro.conversion.py2ri(losses)
vir_wins
#+end_src

#+RESULTS:
:RESULTS:
|     | 0    | 1   | 2   | 3    |
|-----+------+-----+-----+------|
| 0   | 11.2 | 2.0 | 1.9 | 7.8  |
| 1   | 19.2 | 6.0 | 5.1 | 8.6  |
| 2   | 12.9 | 1.0 | 0.8 | 7.3  |
| 3   | 14.6 | 2.0 | 3.9 | 2.3  |
| 4   | 21.0 | 1.0 | 1.7 | 2.2  |
| ... | ...  | ... | ... | ...  |
| 218 | 16.9 | 8.0 | 6.7 | 14.8 |
| 219 | 16.7 | 6.0 | 3.5 | 5.9  |
| 220 | 21.5 | 3.0 | 4.0 | 3.2  |
| 221 | 23.3 | 0.0 | 0.9 | 7.6  |
| 222 | 14.8 | 1.0 | 1.2 | 1.0  |

223 rows × 4 columns
:END:

#+begin_src jupyter-python
def simulate_T2(i, num_perms):
    wins = pfwins_cl.query("CLUSTER=={}".format(i))[cluster_vars]
    losses = pflosses_cl.query("CLUSTER=={}".format(i))[cluster_vars]
    test_stat = T2(wins, losses)

    sample = [] # should pool.map this
    for n in range(num_perms):
        both = np.concatenate([wins, losses])
        both = np.random.permutation(both)
        # T2 needs a pandas df; no numpy arrays allowed
        vir_wins, vir_losses = map(pd.DataFrame, np.split(both, [len(wins)]))
        sample += [T2(vir_wins, vir_losses)]
    sample = pd.Series(sample)
    return test_stat, sample
#+end_src

#+RESULTS:

#+begin_src jupyter-python
%%time
test_stat, sample = simulate_T2(i, 1000)
#+end_src

#+RESULTS:
: CPU times: user 3.93 s, sys: 9.7 ms, total: 3.94 s
: Wall time: 3.95 s


#+begin_src jupyter-python
%%time
from statsmodels.distributions.empirical_distribution import ECDF

num_perms = 1000
for i in pf_cl.CLUSTER.unique():
    print("Cluster: {}".format(i))
    
    test_stat, sample = simulate_T2(i, num_perms)
    mu = sample.mean()
    sample_cdf = ECDF(sample)
    pval = 1 - sample_cdf(test_stat)

    print("p = {}\n".format(pval))
#+end_src

#+RESULTS:
#+begin_example
Cluster: 0
p = 0.06799999999999995

Cluster: 1
p = 0.99

Cluster: 3
p = 0.548

Cluster: 2
p = 0.361

CPU times: user 15.2 s, sys: 72.8 ms, total: 15.2 s
Wall time: 15.3 s
#+end_example


#+begin_src jupyter-python :file sample.png
print(f"Test statistic: {test_stat}")
_ = sns.distplot(sample)
#+end_src

#+RESULTS:
:RESULTS:
: Test statistic: 4.54766581689085
[[file:sample.png]]
:END:

** 1-dimensional t-test in each coordinate

#+begin_src jupyter-python
from scipy.stats import ttest_ind
#+end_src

#+RESULTS:

#+begin_src jupyter-python
cl_labels = pf_cl.CLUSTER.unique()
cl_labels.sort()
for cl in cl_labels:
    print("cluster: {}".format(cl))
    for var in cluster_vars:
        wf = pfwins_cl[pfwins_cl.CLUSTER==cl]
        lf = pflosses_cl[pflosses_cl.CLUSTER==cl]
        w = wf[var]
        l = lf[var]
        p = ttest_ind(w,l, equal_var=False).pvalue
        if p < 0.05/len(cluster_vars):  # <- Bonferroni correction
            print("Var: {}; p={}".format(var, p))
#+end_src

#+RESULTS:
: cluster: 0
: Var: DRIBBLES; p=0.0059611983296662535
: Var: TOUCH_TIME; p=0.0026774338219587657
: cluster: 1
: cluster: 2
: cluster: 3

